{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 05: Data Storage and Management\n",
        "**Project:** Turtle Trading Strategy Research  \n",
        "**Author:** Panwei Hu  \n",
        "**Date:** 2025-08-17\n",
        "\n",
        "## Objectives\n",
        "- Implement production-ready data storage for Turtle Trading research\n",
        "- Support both CSV (raw) and Parquet (processed) formats\n",
        "- Create robust I/O utilities for the research pipeline\n",
        "- Establish data versioning and validation framework\n",
        "- Optimize storage for time series analysis and backtesting\n",
        "\n",
        "## Storage Architecture\n",
        "- **Raw Data**: CSV format in `data/raw/` for auditability and human inspection\n",
        "- **Processed Data**: Parquet format in `data/processed/` for performance and compression\n",
        "- **Versioning**: Timestamped filenames with embedded metadata\n",
        "- **Validation**: Comprehensive data integrity checks at every stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üê¢ Turtle Trading Data Storage Setup\n",
            "Project Root: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/turtle_project\n",
            "Raw Data: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/turtle_project/data/raw\n",
            "Processed Data: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/turtle_project/data/processed\n",
            "\n",
            "üìÅ Existing files:\n",
            "   Raw: 18 CSV files\n",
            "   Processed: 5 Parquet files\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import typing as t\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Set up project paths\n",
        "PROJECT_ROOT = pathlib.Path('..').resolve()\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "\n",
        "# Create directories\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(PROJECT_ROOT / '.env')\n",
        "\n",
        "print('üê¢ Turtle Trading Data Storage Setup')\n",
        "print(f'Project Root: {PROJECT_ROOT}')\n",
        "print(f'Raw Data: {RAW_DIR}')\n",
        "print(f'Processed Data: {PROCESSED_DIR}')\n",
        "\n",
        "# Check existing data files\n",
        "raw_files = list(RAW_DIR.glob('*.csv'))\n",
        "processed_files = list(PROCESSED_DIR.glob('*.parquet'))\n",
        "print(f'\\nüìÅ Existing files:')\n",
        "print(f'   Raw: {len(raw_files)} CSV files')\n",
        "print(f'   Processed: {len(processed_files)} Parquet files')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TurtleDataStorage class loaded\n"
          ]
        }
      ],
      "source": [
        "class TurtleDataStorage:\n",
        "    \"\"\"Production-ready data storage utilities for Turtle Trading research\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def detect_format(path: t.Union[str, pathlib.Path]) -> str:\n",
        "        \"\"\"Detect file format from extension\"\"\"\n",
        "        s = str(path).lower()\n",
        "        if s.endswith('.csv'): \n",
        "            return 'csv'\n",
        "        if any(s.endswith(ext) for ext in ['.parquet', '.pq', '.parq']): \n",
        "            return 'parquet'\n",
        "        if s.endswith('.json'):\n",
        "            return 'json'\n",
        "        raise ValueError(f'Unsupported format: {s}. Supported: .csv, .parquet, .json')\n",
        "    \n",
        "    @staticmethod\n",
        "    def detect_date_columns(df: pd.DataFrame) -> list:\n",
        "        \"\"\"Smart detection of date columns for financial data\"\"\"\n",
        "        date_cols = []\n",
        "        for col in df.columns:\n",
        "            if any(pattern in col.lower() for pattern in ['date', 'time', 'timestamp']):\n",
        "                date_cols.append(col)\n",
        "            elif df[col].dtype == 'object':\n",
        "                sample = df[col].dropna().head()\n",
        "                if len(sample) > 0:\n",
        "                    try:\n",
        "                        pd.to_datetime(sample.iloc[0])\n",
        "                        date_cols.append(col)\n",
        "                    except:\n",
        "                        pass\n",
        "        return date_cols\n",
        "    \n",
        "    @staticmethod\n",
        "    def write_data(df: pd.DataFrame, filename: str, data_type: str = 'raw', **kwargs) -> pathlib.Path:\n",
        "        \"\"\"Write DataFrame to appropriate directory with format detection\"\"\"\n",
        "        \n",
        "        # Determine directory based on data type\n",
        "        if data_type == 'raw':\n",
        "            base_dir = RAW_DIR\n",
        "            # Force CSV for raw data\n",
        "            if not filename.endswith('.csv'):\n",
        "                filename = filename.replace('.parquet', '').replace('.json', '') + '.csv'\n",
        "        else:  # processed\n",
        "            base_dir = PROCESSED_DIR\n",
        "            # Prefer Parquet for processed data\n",
        "            if not any(filename.endswith(ext) for ext in ['.parquet', '.csv', '.json']):\n",
        "                filename += '.parquet'\n",
        "        \n",
        "        path = base_dir / filename\n",
        "        fmt = TurtleDataStorage.detect_format(path)\n",
        "        \n",
        "        try:\n",
        "            if fmt == 'csv':\n",
        "                defaults = {'index': False, 'date_format': '%Y-%m-%d'}\n",
        "                df.to_csv(path, **{**defaults, **kwargs})\n",
        "                \n",
        "            elif fmt == 'parquet':\n",
        "                # Try different engines with fallback\n",
        "                engines = ['pyarrow', 'fastparquet']\n",
        "                last_error = None\n",
        "                \n",
        "                for engine in engines:\n",
        "                    try:\n",
        "                        defaults = {'engine': engine, 'compression': 'snappy'}\n",
        "                        df.to_parquet(path, **{**defaults, **kwargs})\n",
        "                        break\n",
        "                    except ImportError as e:\n",
        "                        last_error = e\n",
        "                        continue\n",
        "                    except Exception as e:\n",
        "                        last_error = e\n",
        "                        break\n",
        "                else:\n",
        "                    raise RuntimeError(f'No Parquet engine available. Install pyarrow or fastparquet. Last error: {last_error}')\n",
        "                    \n",
        "            elif fmt == 'json':\n",
        "                defaults = {'orient': 'records', 'date_format': 'iso'}\n",
        "                df.to_json(path, **{**defaults, **kwargs})\n",
        "                \n",
        "            print(f\"üíæ Saved {fmt.upper()}: {path.name} ({path.stat().st_size/1024:.1f} KB)\")\n",
        "            return path\n",
        "            \n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f'Failed to write {fmt.upper()} file {path}: {e}') from e\n",
        "    \n",
        "    @staticmethod\n",
        "    def read_data(path: t.Union[str, pathlib.Path], **kwargs) -> pd.DataFrame:\n",
        "        \"\"\"Read DataFrame with smart parsing for financial data\"\"\"\n",
        "        p = pathlib.Path(path)\n",
        "        \n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(f'File not found: {p}')\n",
        "            \n",
        "        fmt = TurtleDataStorage.detect_format(p)\n",
        "        \n",
        "        try:\n",
        "            if fmt == 'csv':\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\")\n",
        "                    # Smart date parsing\n",
        "                    sample_df = pd.read_csv(p, nrows=0)\n",
        "                    date_cols = TurtleDataStorage.detect_date_columns(sample_df)\n",
        "                    \n",
        "                    defaults = {'parse_dates': date_cols} if date_cols else {}\n",
        "                    df = pd.read_csv(p, **{**defaults, **kwargs})\n",
        "                    \n",
        "            elif fmt == 'parquet':\n",
        "                engines = ['pyarrow', 'fastparquet']\n",
        "                last_error = None\n",
        "                \n",
        "                for engine in engines:\n",
        "                    try:\n",
        "                        defaults = {'engine': engine}\n",
        "                        df = pd.read_parquet(p, **{**defaults, **kwargs})\n",
        "                        break\n",
        "                    except ImportError as e:\n",
        "                        last_error = e\n",
        "                        continue\n",
        "                    except Exception as e:\n",
        "                        last_error = e\n",
        "                        break\n",
        "                else:\n",
        "                    raise RuntimeError(f'No Parquet engine available. Last error: {last_error}')\n",
        "                    \n",
        "            elif fmt == 'json':\n",
        "                df = pd.read_json(p, **kwargs)\n",
        "                \n",
        "            print(f\"üìñ Loaded {fmt.upper()}: {p.name} ‚Üí {df.shape}\")\n",
        "            return df\n",
        "            \n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f'Failed to read {fmt.upper()} file {p}: {e}') from e\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_financial_data(df: pd.DataFrame, required_cols: list = None) -> dict:\n",
        "        \"\"\"Comprehensive validation for financial time series data\"\"\"\n",
        "        \n",
        "        if required_cols is None:\n",
        "            required_cols = ['date', 'symbol', 'adj_close']\n",
        "        \n",
        "        validation = {\n",
        "            'shape': df.shape,\n",
        "            'missing_cols': [c for c in required_cols if c not in df.columns],\n",
        "            'total_nulls': df.isnull().sum().sum(),\n",
        "            'duplicate_rows': df.duplicated().sum(),\n",
        "        }\n",
        "        \n",
        "        # Financial data specific checks\n",
        "        if 'date' in df.columns:\n",
        "            validation['date_range'] = (df['date'].min(), df['date'].max())\n",
        "            validation['date_gaps'] = len(pd.date_range(df['date'].min(), df['date'].max(), freq='D')) - df['date'].nunique()\n",
        "        \n",
        "        if 'symbol' in df.columns:\n",
        "            validation['unique_symbols'] = df['symbol'].nunique()\n",
        "            validation['symbols'] = sorted(df['symbol'].unique().tolist())\n",
        "        \n",
        "        if 'adj_close' in df.columns:\n",
        "            validation['price_stats'] = {\n",
        "                'min': df['adj_close'].min(),\n",
        "                'max': df['adj_close'].max(),\n",
        "                'mean': df['adj_close'].mean(),\n",
        "                'negative_prices': (df['adj_close'] <= 0).sum()\n",
        "            }\n",
        "        \n",
        "        # Overall data quality score\n",
        "        issues = len([v for v in [\n",
        "            validation['missing_cols'], \n",
        "            validation['total_nulls'], \n",
        "            validation['duplicate_rows'],\n",
        "            validation.get('price_stats', {}).get('negative_prices', 0)\n",
        "        ] if v])\n",
        "        \n",
        "        validation['quality_score'] = max(0, 100 - issues * 10)\n",
        "        validation['status'] = 'GOOD' if validation['quality_score'] >= 80 else 'NEEDS_REVIEW'\n",
        "        \n",
        "        return validation\n",
        "\n",
        "print(\"‚úÖ TurtleDataStorage class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Looking for existing turtle data...\n",
            "üìä Loading: turtle_universe_source-yfinance_assets-multi_count-18_20250820-102058.csv\n",
            "üìñ Loaded CSV: turtle_universe_source-yfinance_assets-multi_count-18_20250820-102058.csv ‚Üí (9036, 4)\n",
            "\n",
            "üìã Data Validation Results:\n",
            "   Status: GOOD\n",
            "   Quality Score: 100/100\n",
            "   Shape: (9036, 4)\n",
            "   Date Range: (Timestamp('2023-08-21 00:00:00'), Timestamp('2025-08-20 00:00:00'))\n",
            "   Unique Symbols: 18\n",
            "üíæ Saved PARQUET: turtle_universe_processed_20250820_102603.parquet (70.0 KB)\n",
            "üìñ Loaded PARQUET: turtle_universe_processed_20250820_102603.parquet ‚Üí (9036, 4)\n",
            "\n",
            "üîÑ Round-trip Test: ‚úÖ PASSED\n",
            "\n",
            "üìä Storage Efficiency:\n",
            "   Raw CSV: 394.5 KB\n",
            "   Processed Parquet: 70.0 KB\n",
            "   Compression Ratio: 5.6x\n",
            "\n",
            "‚úÖ Data storage system ready for Turtle Trading research!\n"
          ]
        }
      ],
      "source": [
        "# Load and process existing turtle data if available\n",
        "print(\"üîç Looking for existing turtle data...\")\n",
        "\n",
        "turtle_files = [f for f in raw_files if 'turtle' in f.name.lower()]\n",
        "\n",
        "if turtle_files:\n",
        "    # Load the most recent turtle data file\n",
        "    latest_file = max(turtle_files, key=lambda x: x.stat().st_mtime)\n",
        "    print(f\"üìä Loading: {latest_file.name}\")\n",
        "    \n",
        "    try:\n",
        "        # Load raw data\n",
        "        df_turtle_raw = TurtleDataStorage.read_data(latest_file)\n",
        "        \n",
        "        # Validate the data\n",
        "        validation = TurtleDataStorage.validate_financial_data(df_turtle_raw)\n",
        "        print(f\"\\nüìã Data Validation Results:\")\n",
        "        print(f\"   Status: {validation['status']}\")\n",
        "        print(f\"   Quality Score: {validation['quality_score']}/100\")\n",
        "        print(f\"   Shape: {validation['shape']}\")\n",
        "        print(f\"   Date Range: {validation.get('date_range', 'N/A')}\")\n",
        "        print(f\"   Unique Symbols: {validation.get('unique_symbols', 'N/A')}\")\n",
        "        \n",
        "        if validation['status'] == 'GOOD':\n",
        "            # Save as processed data in Parquet format\n",
        "            timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            processed_filename = f\"turtle_universe_processed_{timestamp}.parquet\"\n",
        "            \n",
        "            processed_path = TurtleDataStorage.write_data(\n",
        "                df_turtle_raw, \n",
        "                processed_filename, \n",
        "                data_type='processed'\n",
        "            )\n",
        "            \n",
        "            # Test round-trip data integrity\n",
        "            df_processed = TurtleDataStorage.read_data(processed_path)\n",
        "            \n",
        "            # Validate round-trip\n",
        "            shapes_match = df_turtle_raw.shape == df_processed.shape\n",
        "            print(f\"\\nüîÑ Round-trip Test: {'‚úÖ PASSED' if shapes_match else '‚ùå FAILED'}\")\n",
        "            \n",
        "            # Performance comparison\n",
        "            raw_size = latest_file.stat().st_size / 1024\n",
        "            processed_size = processed_path.stat().st_size / 1024\n",
        "            compression_ratio = raw_size / processed_size if processed_size > 0 else 0\n",
        "            \n",
        "            print(f\"\\nüìä Storage Efficiency:\")\n",
        "            print(f\"   Raw CSV: {raw_size:.1f} KB\")\n",
        "            print(f\"   Processed Parquet: {processed_size:.1f} KB\")\n",
        "            print(f\"   Compression Ratio: {compression_ratio:.1f}x\")\n",
        "            \n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è  Data quality issues detected. Review required.\")\n",
        "            print(f\"   Missing columns: {validation['missing_cols']}\")\n",
        "            print(f\"   Total nulls: {validation['total_nulls']}\")\n",
        "            print(f\"   Duplicate rows: {validation['duplicate_rows']}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing turtle data: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"üìù No turtle data found. Run 04_data_acquisition.ipynb first to collect data.\")\n",
        "    \n",
        "    # Create sample data for testing\n",
        "    print(\"\\nüß™ Creating sample data for testing...\")\n",
        "    \n",
        "    sample_data = []\n",
        "    symbols = ['SPY', 'QQQ', 'GLD']\n",
        "    dates = pd.date_range('2024-01-01', periods=30, freq='D')\n",
        "    \n",
        "    for symbol in symbols:\n",
        "        for date in dates:\n",
        "            sample_data.append({\n",
        "                'date': date,\n",
        "                'symbol': symbol,\n",
        "                'adj_close': 100 + np.random.randn() * 5,\n",
        "                'asset_category': 'equity_us' if symbol in ['SPY', 'QQQ'] else 'commodities'\n",
        "            })\n",
        "    \n",
        "    df_sample = pd.DataFrame(sample_data)\n",
        "    \n",
        "    # Test storage system with sample data\n",
        "    sample_path = TurtleDataStorage.write_data(df_sample, 'sample_turtle_data.csv', 'raw')\n",
        "    df_loaded = TurtleDataStorage.read_data(sample_path)\n",
        "    \n",
        "    validation_sample = TurtleDataStorage.validate_financial_data(df_loaded)\n",
        "    print(f\"Sample data validation: {validation_sample['status']} ({validation_sample['quality_score']}/100)\")\n",
        "\n",
        "print(\"\\n‚úÖ Data storage system ready for Turtle Trading research!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mfe_bootcamp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
