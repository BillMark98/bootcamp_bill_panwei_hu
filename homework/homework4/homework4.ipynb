{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528dd045",
   "metadata": {},
   "source": [
    "# Homework Starter ‚Äî Stage 04: Data Acquisition and Ingestion\n",
    "Name: Panwei Hu\n",
    "Date: 2025-08-20\n",
    "\n",
    "## Objectives\n",
    "- API ingestion with secrets in `.env`\n",
    "- Scrape a permitted public table\n",
    "- Validate and save raw data to `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85dd1c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/homework/homework4\n",
      ".env file path: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/homework/homework4/.env\n",
      "ALPHAVANTAGE_API_KEY loaded? False\n",
      "Data directory: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/homework/homework4/data/raw\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up local data directory for this homework\n",
    "RAW = pathlib.Path('data/raw')\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"current working directory:\", pathlib.Path.cwd())\n",
    "# Load environment variables\n",
    "print(\".env file path:\", pathlib.Path('.env').absolute())\n",
    "load_dotenv()\n",
    "print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY')))\n",
    "print('Data directory:', RAW.absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508af7d",
   "metadata": {},
   "source": [
    "## Helpers (use or modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e857c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved', path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff960bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dj/t_cw33ws3lb2m3y9lb4jtk640000gn/T/ipykernel_98168/435469504.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(symbol, period='2y', interval='1d', progress=False)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf \n",
    "symbol = 'AAPL'\n",
    "df = yf.download(symbol, period='2y', interval='1d', progress=False)\n",
    "df.head()\n",
    "\n",
    "df.to_csv('AAPL.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95cc42",
   "metadata": {},
   "source": [
    "## Part 1 ‚Äî API Pull (Required)\n",
    "Acquire financial time series data for Turtle Trading strategy analysis.\n",
    "We'll focus on diversified assets: stocks, ETFs, and futures proxies for trend-following analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5310a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting data acquisition for 17 symbols...\n",
      "Using yfinance (Yahoo Finance)\n",
      "Fetching SPY... ‚úÖ 501 records\n",
      "Fetching QQQ... ‚úÖ 501 records\n",
      "Fetching IWM... ‚úÖ 501 records\n",
      "Fetching EFA... ‚úÖ 501 records\n",
      "Fetching EEM... ‚úÖ 501 records\n",
      "Fetching TLT... ‚úÖ 501 records\n",
      "Fetching IEF... ‚úÖ 501 records\n",
      "Fetching LQD... ‚úÖ 501 records\n",
      "Fetching HYG... ‚úÖ 501 records\n",
      "Fetching GLD... ‚úÖ 501 records\n",
      "Fetching SLV... ‚úÖ 501 records\n",
      "Fetching USO... ‚úÖ 501 records\n",
      "Fetching UNG... ‚úÖ 501 records\n",
      "Fetching DBA... ‚úÖ 501 records\n",
      "Fetching FXE... ‚úÖ 501 records\n",
      "Fetching FXY... ‚úÖ 501 records\n",
      "Fetching UUP... ‚úÖ 501 records\n",
      "\n",
      "üéâ SUCCESS! Retrieved 8,517 total records for 17 symbols\n",
      "üìä Validation: {'missing': [], 'shape': (8517, 3), 'na_total': 0}\n",
      "Saved data/raw/api_source-yfinance_assets-multi_count-17_20250820-091757.csv\n",
      "üíæ Saved to: data/raw/api_source-yfinance_assets-multi_count-17_20250820-091757.csv\n",
      "\n",
      "üìà Data Summary by Symbol:\n",
      "             date            adj_close        \n",
      "              min        max     count    mean\n",
      "symbol                                        \n",
      "DBA    2023-08-21 2025-08-19       501   23.88\n",
      "EEM    2023-08-21 2025-08-19       501   41.54\n",
      "EFA    2023-08-21 2025-08-19       501   76.59\n",
      "FXE    2023-08-21 2025-08-19       501   99.07\n",
      "FXY    2023-08-21 2025-08-19       501   61.82\n",
      "GLD    2023-08-21 2025-08-19       501  235.22\n",
      "HYG    2023-08-21 2025-08-19       501   73.52\n",
      "IEF    2023-08-21 2025-08-19       501   90.85\n",
      "IWM    2023-08-21 2025-08-19       501  203.58\n",
      "LQD    2023-08-21 2025-08-19       501  103.10\n",
      "QQQ    2023-08-21 2025-08-19       501  462.24\n",
      "SLV    2023-08-21 2025-08-19       501   26.57\n",
      "SPY    2023-08-21 2025-08-19       501  533.05\n",
      "TLT    2023-08-21 2025-08-19       501   87.58\n",
      "UNG    2023-08-21 2025-08-19       501   18.24\n",
      "USO    2023-08-21 2025-08-19       501   74.15\n",
      "UUP    2023-08-21 2025-08-19       501   27.47\n"
     ]
    }
   ],
   "source": [
    "# FIXED VERSION: Complete data acquisition with corrected yfinance logic\n",
    "SYMBOLS = ['SPY', 'QQQ', 'IWM', 'EFA', 'EEM',  # Equity ETFs\n",
    "           'TLT', 'IEF', 'LQD', 'HYG',          # Bond ETFs  \n",
    "           'GLD', 'SLV', 'USO', 'UNG', 'DBA',   # Commodity ETFs\n",
    "           'FXE', 'FXY', 'UUP']                  # Currency ETFs\n",
    "\n",
    "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "all_data = []\n",
    "\n",
    "print(f\"üöÄ Starting data acquisition for {len(SYMBOLS)} symbols...\")\n",
    "print(f\"Using {'Alpha Vantage API' if USE_ALPHA else 'yfinance (Yahoo Finance)'}\")\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    print(f\"Fetching {symbol}...\", end=' ')\n",
    "    try:\n",
    "        if USE_ALPHA:\n",
    "            # Alpha Vantage API logic\n",
    "            url = 'https://www.alphavantage.co/query'\n",
    "            params = {\n",
    "                'function': 'TIME_SERIES_DAILY_ADJUSTED',\n",
    "                'symbol': symbol,\n",
    "                'outputsize': 'full',\n",
    "                'apikey': os.getenv('ALPHAVANTAGE_API_KEY')\n",
    "            }\n",
    "            r = requests.get(url, params=params, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            js = r.json()\n",
    "            \n",
    "            if 'Error Message' in js:\n",
    "                print(f\"‚ùå API Error: {js['Error Message']}\")\n",
    "                continue\n",
    "            if 'Note' in js:\n",
    "                print(f\"‚ö†Ô∏è  API Limit: {js['Note']}\")\n",
    "                continue\n",
    "                \n",
    "            key = [k for k in js if 'Time Series' in k][0]\n",
    "            df = pd.DataFrame(js[key]).T.reset_index()\n",
    "            df = df.rename(columns={'index':'date','5. adjusted close':'adj_close'})\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df['adj_close'] = pd.to_numeric(df['adj_close'])\n",
    "            df = df[['date','adj_close']].copy()\n",
    "            \n",
    "        else:\n",
    "            # Fixed yfinance logic\n",
    "            import yfinance as yf\n",
    "            df = yf.download(symbol, period='2y', interval='1d', progress=False, auto_adjust=True)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data\")\n",
    "                continue\n",
    "                \n",
    "            df = df.reset_index()\n",
    "            \n",
    "            # With auto_adjust=True, the 'Close' column contains adjusted prices\n",
    "            if 'Close' in df.columns and 'Date' in df.columns:\n",
    "                df = df[['Date', 'Close']].copy()\n",
    "                df.columns = ['date', 'adj_close']\n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected columns: {list(df.columns)}\")\n",
    "                continue\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"‚ùå Empty data\")\n",
    "            continue\n",
    "            \n",
    "        df['symbol'] = symbol\n",
    "        all_data.append(df)\n",
    "        print(f\"‚úÖ {len(df)} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Process results\n",
    "if all_data:\n",
    "    df_api = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nüéâ SUCCESS! Retrieved {len(df_api):,} total records for {df_api['symbol'].nunique()} symbols\")\n",
    "    \n",
    "    # Validation\n",
    "    v_api = validate(df_api, ['date','adj_close','symbol'])\n",
    "    print(f\"üìä Validation: {v_api}\")\n",
    "    \n",
    "    # Save data\n",
    "    df_sorted = df_api.sort_values(['symbol', 'date']).reset_index(drop=True)\n",
    "    saved_path = save_csv(df_sorted, prefix='api', source='alpha' if USE_ALPHA else 'yfinance', \n",
    "                         assets='multi', count=len(SYMBOLS))\n",
    "    print(f\"üíæ Saved to: {saved_path}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nüìà Data Summary by Symbol:\")\n",
    "    summary = df_sorted.groupby('symbol').agg({\n",
    "        'date': ['min', 'max'], \n",
    "        'adj_close': ['count', 'mean']\n",
    "    }).round(2)\n",
    "    print(summary)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data retrieved! Check your internet connection or API keys.\")\n",
    "    df_api = pd.DataFrame(columns=['date', 'adj_close', 'symbol'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f33270",
   "metadata": {},
   "source": [
    "## Part 2 ‚Äî Scrape a Public Table (Required)\n",
    "Scrape additional market data relevant to Turtle Trading. We'll get S&P 500 sector information \n",
    "and volatility data from public financial websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e78bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dj/t_cw33ws3lb2m3y9lb4jtk640000gn/T/ipykernel_98168/1340915299.py:16: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(resp.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success: Found 503 S&P 500 companies\n",
      "\n",
      "Scraped data shape: (503, 3)\n",
      "Sample data:\n",
      "  ticker                 name                  sector\n",
      "0    MMM                   3M             Industrials\n",
      "1    AOS          A. O. Smith             Industrials\n",
      "2    ABT  Abbott Laboratories             Health Care\n",
      "3   ABBV               AbbVie             Health Care\n",
      "4    ACN            Accenture  Information Technology\n",
      "\n",
      "Validation results: {'missing': [], 'shape': (503, 3), 'na_total': 0}\n"
     ]
    }
   ],
   "source": [
    "# Try to scrape S&P 500 sector ETF information from a financial website\n",
    "SCRAPE_URLS = [\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies',  # S&P 500 companies\n",
    "]\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Turtle-Trading-Research/1.0)'}\n",
    "df_scrape = None\n",
    "\n",
    "for url in SCRAPE_URLS:\n",
    "    try:\n",
    "        print(f\"Attempting to scrape: {url}\")\n",
    "        resp = requests.get(url, headers=headers, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        # Use pandas to read HTML tables directly\n",
    "        tables = pd.read_html(resp.text)\n",
    "        \n",
    "        if 'S%26P_500_companies' in url:\n",
    "            # Get S&P 500 companies table\n",
    "            df_scrape = tables[0]  # First table usually contains the companies\n",
    "            df_scrape = df_scrape[['Symbol', 'Security', 'GICS Sector']].copy()\n",
    "            df_scrape.columns = ['ticker', 'name', 'sector']\n",
    "            print(f\"  Success: Found {len(df_scrape)} S&P 500 companies\")\n",
    "            break\n",
    "            \n",
    "        elif 'Sector_ETFs' in url:\n",
    "            # Get sector ETF information\n",
    "            df_scrape = tables[0]  # First table with sector ETFs\n",
    "            if 'Ticker' in df_scrape.columns:\n",
    "                df_scrape = df_scrape[['Ticker', 'Name', 'Sector']].copy()\n",
    "                df_scrape.columns = ['ticker', 'name', 'sector']\n",
    "            print(f\"  Success: Found {len(df_scrape)} sector ETFs\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to scrape {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fallback to demo data if scraping fails\n",
    "if df_scrape is None or df_scrape.empty:\n",
    "    print('Scraping failed, using demo sector ETF data')\n",
    "    demo_data = {\n",
    "        'ticker': ['XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLU', 'XLB', 'XLRE', 'XLC'],\n",
    "        'name': ['Technology', 'Financials', 'Health Care', 'Energy', 'Industrials', \n",
    "                'Consumer Discretionary', 'Consumer Staples', 'Utilities', 'Materials', \n",
    "                'Real Estate', 'Communication Services'],\n",
    "        'sector': ['Technology', 'Financials', 'Health Care', 'Energy', 'Industrials',\n",
    "                  'Consumer Discretionary', 'Consumer Staples', 'Utilities', 'Materials',\n",
    "                  'Real Estate', 'Communication Services']\n",
    "    }\n",
    "    df_scrape = pd.DataFrame(demo_data)\n",
    "\n",
    "print(f\"\\nScraped data shape: {df_scrape.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df_scrape.head())\n",
    "\n",
    "v_scrape = validate(df_scrape, ['ticker', 'name', 'sector'])\n",
    "print(f\"\\nValidation results: {v_scrape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245d816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/raw/scrape_site-wikipedia_table-sp500_sectors_20250820-091903.csv\n",
      "Saved scraped data to data/raw/scrape_site-wikipedia_table-sp500_sectors_20250820-091903.csv\n"
     ]
    }
   ],
   "source": [
    "# Save scraped data\n",
    "if not df_scrape.empty:\n",
    "    saved_path = save_csv(df_scrape, prefix='scrape', site='wikipedia', table='sp500_sectors')\n",
    "    print(f\"Saved scraped data to {saved_path}\")\n",
    "else:\n",
    "    print(\"No scraped data to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ebd094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA ACQUISITION SUMMARY ===\n",
      "Timestamp: 2025-08-17 21:17:20.678184\n",
      "Data directory: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/homework/../turtle_project/data/raw\n",
      "\n",
      "üìà API Data Summary:\n",
      "  - Total records: 8,534\n",
      "  - Unique symbols: 17\n",
      "  - Date range: 2023-08-16 00:00:00 to 2025-08-15 00:00:00\n",
      "  - Missing values: 0\n",
      "\n",
      "  Latest prices (sample):\n",
      "    DBA: $27.06\n",
      "    EEM: $49.94\n",
      "    EFA: $92.19\n",
      "    FXE: $107.96\n",
      "    FXY: $62.54\n",
      "\n",
      "üîç Scraped Data Summary:\n",
      "  - Records: 503\n",
      "  - Sectors: 11\n",
      "\n",
      "üíæ Files saved to: ../turtle_project/data/raw\n",
      "‚úÖ Data acquisition complete!\n",
      "\n",
      "üìÅ Raw data files (5):\n",
      "  - api_source-yfinance_assets-multi_count-17_20250817-211709.csv (0.3 MB)\n",
      "  - api_source-yfinance_assets-multi_count-17_20250817-211655.csv (0.3 MB)\n",
      "  - scrape_site-wikipedia_table-sp500_sectors_20250817-205825.csv (0.0 MB)\n",
      "  - scrape_site-wikipedia_table-sp500_sectors_20250817-211718.csv (0.0 MB)\n",
      "  - api_fixed_source-yfinance_assets-multi_count-17_20250817-211626.csv (0.3 MB)\n"
     ]
    }
   ],
   "source": [
    "# Additional Data Quality Checks and Summary\n",
    "print(\"=== DATA ACQUISITION SUMMARY ===\")\n",
    "print(f\"Timestamp: {dt.datetime.now()}\")\n",
    "print(f\"Data directory: {RAW.absolute()}\")\n",
    "\n",
    "if not df_api.empty:\n",
    "    print(f\"\\nüìà API Data Summary:\")\n",
    "    print(f\"  - Total records: {len(df_api):,}\")\n",
    "    print(f\"  - Unique symbols: {df_api['symbol'].nunique()}\")\n",
    "    print(f\"  - Date range: {df_api['date'].min()} to {df_api['date'].max()}\")\n",
    "    print(f\"  - Missing values: {df_api.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    latest_prices = df_api.groupby('symbol')['adj_close'].last()\n",
    "    print(f\"\\n  Latest prices (sample):\")\n",
    "    for symbol in latest_prices.head(5).index:\n",
    "        price = latest_prices[symbol]\n",
    "        print(f\"    {symbol}: ${price:.2f}\")\n",
    "\n",
    "if not df_scrape.empty:\n",
    "    print(f\"\\nüîç Scraped Data Summary:\")\n",
    "    print(f\"  - Records: {len(df_scrape)}\")\n",
    "    print(f\"  - Sectors: {df_scrape['sector'].nunique() if 'sector' in df_scrape.columns else 'N/A'}\")\n",
    "    \n",
    "print(f\"\\nüíæ Files saved to: {RAW}\")\n",
    "print(\"‚úÖ Data acquisition complete!\")\n",
    "\n",
    "# Check if turtle_project data directory was created\n",
    "if RAW.exists():\n",
    "    files = list(RAW.glob(\"*.csv\"))\n",
    "    print(f\"\\nüìÅ Raw data files ({len(files)}):\")\n",
    "    for f in files:\n",
    "        size_mb = f.stat().st_size / (1024*1024)\n",
    "        print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå Data directory not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### Data Sources for Turtle Trading Project\n",
    "\n",
    "#### API Sources:\n",
    "- **Primary**: Alpha Vantage API (https://www.alphavantage.co/query)\n",
    "  - Endpoint: TIME_SERIES_DAILY_ADJUSTED\n",
    "  - Parameters: symbol, outputsize=full, apikey\n",
    "  - Fallback: yfinance library for Yahoo Finance data\n",
    "  - Assets: 17 diversified ETFs covering equities, bonds, commodities, currencies\n",
    "  - Time period: Up to 2 years of daily data\n",
    "\n",
    "#### Web Scraping Sources:\n",
    "- **S&P 500 Companies**: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
    "- **Sector ETFs**: https://en.wikipedia.org/wiki/SPDR_Select_Sector_ETFs\n",
    "- **Method**: pandas.read_html() for structured table extraction\n",
    "- **Fallback**: Hardcoded sector ETF data if scraping fails\n",
    "\n",
    "### Asset Universe for Turtle Trading:\n",
    "- **Equity ETFs**: SPY, QQQ, IWM, EFA, EEM (US large/small cap, international)\n",
    "- **Bond ETFs**: TLT, IEF, LQD, HYG (treasury, corporate, high yield)\n",
    "- **Commodity ETFs**: GLD, SLV, USO, UNG, DBA (metals, energy, agriculture)\n",
    "- **Currency ETFs**: FXE, FXY, UUP (euro, yen, dollar strength)\n",
    "\n",
    "### Assumptions & Risks:\n",
    "\n",
    "#### Rate Limits & API Constraints:\n",
    "- Alpha Vantage: 5 calls/minute, 500 calls/day (free tier)\n",
    "- Wikipedia: No formal rate limits, but respectful scraping with delays\n",
    "- yfinance: Unofficial API, subject to Yahoo Finance changes\n",
    "\n",
    "#### Data Quality Risks:\n",
    "- **Schema changes**: Wikipedia table structures may change\n",
    "- **Selector fragility**: HTML parsing depends on consistent table structure  \n",
    "- **Missing data**: Some ETFs may have limited history or gaps\n",
    "- **Corporate actions**: Splits/dividends handled differently across sources\n",
    "\n",
    "#### Operational Risks:\n",
    "- **API key management**: Stored in .env file (excluded from git)\n",
    "- **Network failures**: Timeout handling and retry logic needed\n",
    "- **Data staleness**: Daily data may have 1-day lag\n",
    "- **Cost sensitivity**: Slippage assumptions critical for Turtle strategy viability\n",
    "\n",
    "### Validation & Quality Control:\n",
    "- **Required columns**: date, adj_close, symbol for API data\n",
    "- **Data completeness**: Check for missing values and date gaps\n",
    "- **Date range validation**: Ensure sufficient history for backtesting\n",
    "- **Price reasonableness**: Sanity checks on price levels and returns\n",
    "\n",
    "### Environment Setup:\n",
    "- ‚úÖ `.env` file excluded from git via .gitignore\n",
    "- ‚úÖ Data saved to turtle_project/data/raw/ directory\n",
    "- ‚úÖ Fallback mechanisms for both API and scraping failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfe_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
