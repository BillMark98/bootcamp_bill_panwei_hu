{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 04: Data Acquisition and Ingestion\n",
    "Name: Panwei Hu\n",
    "Date: 2025-01-27\n",
    "\n",
    "## Objectives\n",
    "- API ingestion with secrets in `.env`\n",
    "- Scrape a permitted public table\n",
    "- Validate and save raw data to `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPHAVANTAGE_API_KEY loaded? True\n",
      "Data directory: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/homework/../turtle_project/data/raw\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up data directory for turtle_project\n",
    "TURTLE_ROOT = pathlib.Path('../turtle_project')\n",
    "RAW = TURTLE_ROOT / 'data/raw'\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY')))\n",
    "print('Data directory:', RAW.absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers (use or modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved', path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff960bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf \n",
    "symbol = 'AAPL'\n",
    "df = yf.download(symbol, period='2y', interval='1d', progress=False)\n",
    "df.head()\n",
    "\n",
    "df.to_csv('AAPL.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — API Pull (Required)\n",
    "Acquire financial time series data for Turtle Trading strategy analysis.\n",
    "We'll focus on diversified assets: stocks, ETFs, and futures proxies for trend-following analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f9063ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-16</td>\n",
       "      <td>428.241608</td>\n",
       "      <td>SPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>424.978516</td>\n",
       "      <td>SPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>425.183014</td>\n",
       "      <td>SPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>427.949371</td>\n",
       "      <td>SPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>426.790253</td>\n",
       "      <td>SPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>27.540001</td>\n",
       "      <td>UUP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8530</th>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>UUP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8531</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>27.330000</td>\n",
       "      <td>UUP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8532</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>27.430000</td>\n",
       "      <td>UUP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>27.340000</td>\n",
       "      <td>UUP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8534 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   adj_close symbol\n",
       "0    2023-08-16  428.241608    SPY\n",
       "1    2023-08-17  424.978516    SPY\n",
       "2    2023-08-18  425.183014    SPY\n",
       "3    2023-08-21  427.949371    SPY\n",
       "4    2023-08-22  426.790253    SPY\n",
       "...         ...         ...    ...\n",
       "8529 2025-08-11   27.540001    UUP\n",
       "8530 2025-08-12   27.400000    UUP\n",
       "8531 2025-08-13   27.330000    UUP\n",
       "8532 2025-08-14   27.430000    UUP\n",
       "8533 2025-08-15   27.340000    UUP\n",
       "\n",
       "[8534 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5310a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting data acquisition for 17 symbols...\n",
      "Using yfinance (Yahoo Finance)\n",
      "Fetching SPY... ✅ 502 records\n",
      "Fetching QQQ... ✅ 502 records\n",
      "Fetching IWM... ✅ 502 records\n",
      "Fetching EFA... ✅ 502 records\n",
      "Fetching EEM... ✅ 502 records\n",
      "Fetching TLT... ✅ 502 records\n",
      "Fetching IEF... ✅ 502 records\n",
      "Fetching LQD... ✅ 502 records\n",
      "Fetching HYG... ✅ 502 records\n",
      "Fetching GLD... ✅ 502 records\n",
      "Fetching SLV... ✅ 502 records\n",
      "Fetching USO... ✅ 502 records\n",
      "Fetching UNG... ✅ 502 records\n",
      "Fetching DBA... ✅ 502 records\n",
      "Fetching FXE... ✅ 502 records\n",
      "Fetching FXY... ✅ 502 records\n",
      "Fetching UUP... ✅ 502 records\n",
      "\n",
      "🎉 SUCCESS! Retrieved 8,534 total records for 17 symbols\n",
      "📊 Validation: {'missing': [], 'shape': (8534, 3), 'na_total': 0}\n",
      "Saved ../turtle_project/data/raw/api_source-yfinance_assets-multi_count-17_20250817-211655.csv\n",
      "💾 Saved to: ../turtle_project/data/raw/api_source-yfinance_assets-multi_count-17_20250817-211655.csv\n",
      "\n",
      "📈 Data Summary by Symbol:\n",
      "             date            adj_close        \n",
      "              min        max     count    mean\n",
      "symbol                                        \n",
      "DBA    2023-08-16 2025-08-15       502   23.84\n",
      "EEM    2023-08-16 2025-08-15       502   41.48\n",
      "EFA    2023-08-16 2025-08-15       502   76.46\n",
      "FXE    2023-08-16 2025-08-15       502   99.02\n",
      "FXY    2023-08-16 2025-08-15       502   61.83\n",
      "GLD    2023-08-16 2025-08-15       502  234.58\n",
      "HYG    2023-08-16 2025-08-15       502   73.44\n",
      "IEF    2023-08-16 2025-08-15       502   90.81\n",
      "IWM    2023-08-16 2025-08-15       502  203.35\n",
      "LQD    2023-08-16 2025-08-15       502  103.03\n",
      "QQQ    2023-08-16 2025-08-15       502  461.16\n",
      "SLV    2023-08-16 2025-08-15       502   26.50\n",
      "SPY    2023-08-16 2025-08-15       502  531.98\n",
      "TLT    2023-08-16 2025-08-15       502   87.58\n",
      "UNG    2023-08-16 2025-08-15       502   18.32\n",
      "USO    2023-08-16 2025-08-15       502   74.14\n",
      "UUP    2023-08-16 2025-08-15       502   27.46\n"
     ]
    }
   ],
   "source": [
    "# FIXED VERSION: Complete data acquisition with corrected yfinance logic\n",
    "SYMBOLS = ['SPY', 'QQQ', 'IWM', 'EFA', 'EEM',  # Equity ETFs\n",
    "           'TLT', 'IEF', 'LQD', 'HYG',          # Bond ETFs  \n",
    "           'GLD', 'SLV', 'USO', 'UNG', 'DBA',   # Commodity ETFs\n",
    "           'FXE', 'FXY', 'UUP']                  # Currency ETFs\n",
    "\n",
    "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "all_data = []\n",
    "\n",
    "print(f\"🚀 Starting data acquisition for {len(SYMBOLS)} symbols...\")\n",
    "print(f\"Using {'Alpha Vantage API' if USE_ALPHA else 'yfinance (Yahoo Finance)'}\")\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    print(f\"Fetching {symbol}...\", end=' ')\n",
    "    try:\n",
    "        if USE_ALPHA:\n",
    "            # Alpha Vantage API logic\n",
    "            url = 'https://www.alphavantage.co/query'\n",
    "            params = {\n",
    "                'function': 'TIME_SERIES_DAILY_ADJUSTED',\n",
    "                'symbol': symbol,\n",
    "                'outputsize': 'full',\n",
    "                'apikey': os.getenv('ALPHAVANTAGE_API_KEY')\n",
    "            }\n",
    "            r = requests.get(url, params=params, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            js = r.json()\n",
    "            \n",
    "            if 'Error Message' in js:\n",
    "                print(f\"❌ API Error: {js['Error Message']}\")\n",
    "                continue\n",
    "            if 'Note' in js:\n",
    "                print(f\"⚠️  API Limit: {js['Note']}\")\n",
    "                continue\n",
    "                \n",
    "            key = [k for k in js if 'Time Series' in k][0]\n",
    "            df = pd.DataFrame(js[key]).T.reset_index()\n",
    "            df = df.rename(columns={'index':'date','5. adjusted close':'adj_close'})\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df['adj_close'] = pd.to_numeric(df['adj_close'])\n",
    "            df = df[['date','adj_close']].copy()\n",
    "            \n",
    "        else:\n",
    "            # Fixed yfinance logic\n",
    "            import yfinance as yf\n",
    "            df = yf.download(symbol, period='2y', interval='1d', progress=False, auto_adjust=True)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"❌ No data\")\n",
    "                continue\n",
    "                \n",
    "            df = df.reset_index()\n",
    "            \n",
    "            # With auto_adjust=True, the 'Close' column contains adjusted prices\n",
    "            if 'Close' in df.columns and 'Date' in df.columns:\n",
    "                df = df[['Date', 'Close']].copy()\n",
    "                df.columns = ['date', 'adj_close']\n",
    "            else:\n",
    "                print(f\"❌ Unexpected columns: {list(df.columns)}\")\n",
    "                continue\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"❌ Empty data\")\n",
    "            continue\n",
    "            \n",
    "        df['symbol'] = symbol\n",
    "        all_data.append(df)\n",
    "        print(f\"✅ {len(df)} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Process results\n",
    "if all_data:\n",
    "    df_api = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\n🎉 SUCCESS! Retrieved {len(df_api):,} total records for {df_api['symbol'].nunique()} symbols\")\n",
    "    \n",
    "    # Validation\n",
    "    v_api = validate(df_api, ['date','adj_close','symbol'])\n",
    "    print(f\"📊 Validation: {v_api}\")\n",
    "    \n",
    "    # Save data\n",
    "    df_sorted = df_api.sort_values(['symbol', 'date']).reset_index(drop=True)\n",
    "    saved_path = save_csv(df_sorted, prefix='api', source='alpha' if USE_ALPHA else 'yfinance', \n",
    "                         assets='multi', count=len(SYMBOLS))\n",
    "    print(f\"💾 Saved to: {saved_path}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\n📈 Data Summary by Symbol:\")\n",
    "    summary = df_sorted.groupby('symbol').agg({\n",
    "        'date': ['min', 'max'], \n",
    "        'adj_close': ['count', 'mean']\n",
    "    }).round(2)\n",
    "    print(summary)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data retrieved! Check your internet connection or API keys.\")\n",
    "    df_api = pd.DataFrame(columns=['date', 'adj_close', 'symbol'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — Scrape a Public Table (Required)\n",
    "Scrape additional market data relevant to Turtle Trading. We'll get S&P 500 sector information \n",
    "and volatility data from public financial websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
      "  Success: Found 503 S&P 500 companies\n",
      "\n",
      "Scraped data shape: (503, 3)\n",
      "Sample data:\n",
      "  ticker                 name                  sector\n",
      "0    MMM                   3M             Industrials\n",
      "1    AOS          A. O. Smith             Industrials\n",
      "2    ABT  Abbott Laboratories             Health Care\n",
      "3   ABBV               AbbVie             Health Care\n",
      "4    ACN            Accenture  Information Technology\n",
      "\n",
      "Validation results: {'missing': [], 'shape': (503, 3), 'na_total': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dj/t_cw33ws3lb2m3y9lb4jtk640000gn/T/ipykernel_26418/4049906531.py:17: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(resp.text)\n"
     ]
    }
   ],
   "source": [
    "# Try to scrape S&P 500 sector ETF information from a financial website\n",
    "SCRAPE_URLS = [\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies',  # S&P 500 companies\n",
    "    'https://en.wikipedia.org/wiki/SPDR_Select_Sector_ETFs'      # Sector ETFs\n",
    "]\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Turtle-Trading-Research/1.0)'}\n",
    "df_scrape = None\n",
    "\n",
    "for url in SCRAPE_URLS:\n",
    "    try:\n",
    "        print(f\"Attempting to scrape: {url}\")\n",
    "        resp = requests.get(url, headers=headers, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        # Use pandas to read HTML tables directly\n",
    "        tables = pd.read_html(resp.text)\n",
    "        \n",
    "        if 'S%26P_500_companies' in url:\n",
    "            # Get S&P 500 companies table\n",
    "            df_scrape = tables[0]  # First table usually contains the companies\n",
    "            df_scrape = df_scrape[['Symbol', 'Security', 'GICS Sector']].copy()\n",
    "            df_scrape.columns = ['ticker', 'name', 'sector']\n",
    "            print(f\"  Success: Found {len(df_scrape)} S&P 500 companies\")\n",
    "            break\n",
    "            \n",
    "        elif 'Sector_ETFs' in url:\n",
    "            # Get sector ETF information\n",
    "            df_scrape = tables[0]  # First table with sector ETFs\n",
    "            if 'Ticker' in df_scrape.columns:\n",
    "                df_scrape = df_scrape[['Ticker', 'Name', 'Sector']].copy()\n",
    "                df_scrape.columns = ['ticker', 'name', 'sector']\n",
    "            print(f\"  Success: Found {len(df_scrape)} sector ETFs\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to scrape {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fallback to demo data if scraping fails\n",
    "if df_scrape is None or df_scrape.empty:\n",
    "    print('Scraping failed, using demo sector ETF data')\n",
    "    demo_data = {\n",
    "        'ticker': ['XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLU', 'XLB', 'XLRE', 'XLC'],\n",
    "        'name': ['Technology', 'Financials', 'Health Care', 'Energy', 'Industrials', \n",
    "                'Consumer Discretionary', 'Consumer Staples', 'Utilities', 'Materials', \n",
    "                'Real Estate', 'Communication Services'],\n",
    "        'sector': ['Technology', 'Financials', 'Health Care', 'Energy', 'Industrials',\n",
    "                  'Consumer Discretionary', 'Consumer Staples', 'Utilities', 'Materials',\n",
    "                  'Real Estate', 'Communication Services']\n",
    "    }\n",
    "    df_scrape = pd.DataFrame(demo_data)\n",
    "\n",
    "print(f\"\\nScraped data shape: {df_scrape.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df_scrape.head())\n",
    "\n",
    "v_scrape = validate(df_scrape, ['ticker', 'name', 'sector'])\n",
    "print(f\"\\nValidation results: {v_scrape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../turtle_project/data/raw/scrape_site-wikipedia_table-sp500_sectors_20250817-211718.csv\n",
      "Saved scraped data to ../turtle_project/data/raw/scrape_site-wikipedia_table-sp500_sectors_20250817-211718.csv\n"
     ]
    }
   ],
   "source": [
    "# Save scraped data\n",
    "if not df_scrape.empty:\n",
    "    saved_path = save_csv(df_scrape, prefix='scrape', site='wikipedia', table='sp500_sectors')\n",
    "    print(f\"Saved scraped data to {saved_path}\")\n",
    "else:\n",
    "    print(\"No scraped data to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ebd094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA ACQUISITION SUMMARY ===\n",
      "Timestamp: 2025-08-17 21:17:20.678184\n",
      "Data directory: /Users/panweihu/Desktop/Desktop_m1/NYU_mfe/bootcamp/camp4/bootcamp_bill_panwei_hu/homework/../turtle_project/data/raw\n",
      "\n",
      "📈 API Data Summary:\n",
      "  - Total records: 8,534\n",
      "  - Unique symbols: 17\n",
      "  - Date range: 2023-08-16 00:00:00 to 2025-08-15 00:00:00\n",
      "  - Missing values: 0\n",
      "\n",
      "  Latest prices (sample):\n",
      "    DBA: $27.06\n",
      "    EEM: $49.94\n",
      "    EFA: $92.19\n",
      "    FXE: $107.96\n",
      "    FXY: $62.54\n",
      "\n",
      "🔍 Scraped Data Summary:\n",
      "  - Records: 503\n",
      "  - Sectors: 11\n",
      "\n",
      "💾 Files saved to: ../turtle_project/data/raw\n",
      "✅ Data acquisition complete!\n",
      "\n",
      "📁 Raw data files (5):\n",
      "  - api_source-yfinance_assets-multi_count-17_20250817-211709.csv (0.3 MB)\n",
      "  - api_source-yfinance_assets-multi_count-17_20250817-211655.csv (0.3 MB)\n",
      "  - scrape_site-wikipedia_table-sp500_sectors_20250817-205825.csv (0.0 MB)\n",
      "  - scrape_site-wikipedia_table-sp500_sectors_20250817-211718.csv (0.0 MB)\n",
      "  - api_fixed_source-yfinance_assets-multi_count-17_20250817-211626.csv (0.3 MB)\n"
     ]
    }
   ],
   "source": [
    "# Additional Data Quality Checks and Summary\n",
    "print(\"=== DATA ACQUISITION SUMMARY ===\")\n",
    "print(f\"Timestamp: {dt.datetime.now()}\")\n",
    "print(f\"Data directory: {RAW.absolute()}\")\n",
    "\n",
    "if not df_api.empty:\n",
    "    print(f\"\\n📈 API Data Summary:\")\n",
    "    print(f\"  - Total records: {len(df_api):,}\")\n",
    "    print(f\"  - Unique symbols: {df_api['symbol'].nunique()}\")\n",
    "    print(f\"  - Date range: {df_api['date'].min()} to {df_api['date'].max()}\")\n",
    "    print(f\"  - Missing values: {df_api.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    latest_prices = df_api.groupby('symbol')['adj_close'].last()\n",
    "    print(f\"\\n  Latest prices (sample):\")\n",
    "    for symbol in latest_prices.head(5).index:\n",
    "        price = latest_prices[symbol]\n",
    "        print(f\"    {symbol}: ${price:.2f}\")\n",
    "\n",
    "if not df_scrape.empty:\n",
    "    print(f\"\\n🔍 Scraped Data Summary:\")\n",
    "    print(f\"  - Records: {len(df_scrape)}\")\n",
    "    print(f\"  - Sectors: {df_scrape['sector'].nunique() if 'sector' in df_scrape.columns else 'N/A'}\")\n",
    "    \n",
    "print(f\"\\n💾 Files saved to: {RAW}\")\n",
    "print(\"✅ Data acquisition complete!\")\n",
    "\n",
    "# Check if turtle_project data directory was created\n",
    "if RAW.exists():\n",
    "    files = list(RAW.glob(\"*.csv\"))\n",
    "    print(f\"\\n📁 Raw data files ({len(files)}):\")\n",
    "    for f in files:\n",
    "        size_mb = f.stat().st_size / (1024*1024)\n",
    "        print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"❌ Data directory not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### Data Sources for Turtle Trading Project\n",
    "\n",
    "#### API Sources:\n",
    "- **Primary**: Alpha Vantage API (https://www.alphavantage.co/query)\n",
    "  - Endpoint: TIME_SERIES_DAILY_ADJUSTED\n",
    "  - Parameters: symbol, outputsize=full, apikey\n",
    "  - Fallback: yfinance library for Yahoo Finance data\n",
    "  - Assets: 17 diversified ETFs covering equities, bonds, commodities, currencies\n",
    "  - Time period: Up to 2 years of daily data\n",
    "\n",
    "#### Web Scraping Sources:\n",
    "- **S&P 500 Companies**: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
    "- **Sector ETFs**: https://en.wikipedia.org/wiki/SPDR_Select_Sector_ETFs\n",
    "- **Method**: pandas.read_html() for structured table extraction\n",
    "- **Fallback**: Hardcoded sector ETF data if scraping fails\n",
    "\n",
    "### Asset Universe for Turtle Trading:\n",
    "- **Equity ETFs**: SPY, QQQ, IWM, EFA, EEM (US large/small cap, international)\n",
    "- **Bond ETFs**: TLT, IEF, LQD, HYG (treasury, corporate, high yield)\n",
    "- **Commodity ETFs**: GLD, SLV, USO, UNG, DBA (metals, energy, agriculture)\n",
    "- **Currency ETFs**: FXE, FXY, UUP (euro, yen, dollar strength)\n",
    "\n",
    "### Assumptions & Risks:\n",
    "\n",
    "#### Rate Limits & API Constraints:\n",
    "- Alpha Vantage: 5 calls/minute, 500 calls/day (free tier)\n",
    "- Wikipedia: No formal rate limits, but respectful scraping with delays\n",
    "- yfinance: Unofficial API, subject to Yahoo Finance changes\n",
    "\n",
    "#### Data Quality Risks:\n",
    "- **Schema changes**: Wikipedia table structures may change\n",
    "- **Selector fragility**: HTML parsing depends on consistent table structure  \n",
    "- **Missing data**: Some ETFs may have limited history or gaps\n",
    "- **Corporate actions**: Splits/dividends handled differently across sources\n",
    "\n",
    "#### Operational Risks:\n",
    "- **API key management**: Stored in .env file (excluded from git)\n",
    "- **Network failures**: Timeout handling and retry logic needed\n",
    "- **Data staleness**: Daily data may have 1-day lag\n",
    "- **Cost sensitivity**: Slippage assumptions critical for Turtle strategy viability\n",
    "\n",
    "### Validation & Quality Control:\n",
    "- **Required columns**: date, adj_close, symbol for API data\n",
    "- **Data completeness**: Check for missing values and date gaps\n",
    "- **Date range validation**: Ensure sufficient history for backtesting\n",
    "- **Price reasonableness**: Sanity checks on price levels and returns\n",
    "\n",
    "### Environment Setup:\n",
    "- ✅ `.env` file excluded from git via .gitignore\n",
    "- ✅ Data saved to turtle_project/data/raw/ directory\n",
    "- ✅ Fallback mechanisms for both API and scraping failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
